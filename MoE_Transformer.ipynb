{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WilliamZhang20/Transformer-from-Scratch/blob/main/MoE_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building a GPT\n",
        "\n",
        "Added KV Caching + Mixture of Experts"
      ],
      "metadata": {
        "id": "wJpXpmjEYC_T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notes:\n",
        "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
        "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
        "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
        "- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
        "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
        "- \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
      ],
      "metadata": {
        "id": "M5CvobiQ0pLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer # Use a real tokenizer, e.g., a simple one for char-level/word-level\n",
        "\n",
        "# --- Setup for Tokenization and Block Processing ---\n",
        "# Use the same block_size from your model\n",
        "block_size = 32\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# A simple tokenizer (e.g., a GPT-2 style BPE tokenizer for illustration)\n",
        "# For simplicity and scale, let's use a standard word/subword tokenizer.\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", use_fast=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token # Necessary for batching\n",
        "\n",
        "vocab_size = tokenizer.vocab_size\n",
        "\n",
        "# --- 1. Load the Dataset ---\n",
        "print(\"Loading WikiText-103 dataset...\")\n",
        "raw_datasets = load_dataset(\"wikitext\", \"wikitext-103-v1\")\n",
        "\n",
        "# --- 2. Tokenize the Data ---\n",
        "def tokenize_function(examples):\n",
        "    # 'text' is the column name in the wikitext dataset\n",
        "    return tokenizer(examples[\"text\"], truncation=False, max_length=100000)\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    num_proc=4, # Use multiple processes for fast tokenization\n",
        "    remove_columns=[\"text\"],\n",
        ")\n",
        "print(\"Tokenization complete.\")\n",
        "\n",
        "# --- 3. Group and Block the Data (Crucial for LM Training) ---\n",
        "# Concatenate all texts and split them into fixed-size chunks (block_size)\n",
        "def group_texts(examples):\n",
        "    # Concatenate all texts.\n",
        "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
        "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
        "\n",
        "    # Drop the last chunk if it's smaller than block_size\n",
        "    total_length = (total_length // block_size) * block_size\n",
        "\n",
        "    # Split by block_size\n",
        "    result = {\n",
        "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
        "        for k, t in concatenated_examples.items()\n",
        "    }\n",
        "\n",
        "    # 'labels' (targets) are just the shifted input 'input_ids'\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result\n",
        "\n",
        "lm_datasets = tokenized_datasets.map(\n",
        "    group_texts,\n",
        "    batched=True,\n",
        "    num_proc=4,\n",
        ")\n",
        "print(\"Grouping complete.\")\n",
        "\n",
        "\n",
        "# --- 4. Custom DataLoader/get_batch for the Model ---\n",
        "train_data = lm_datasets['train'].with_format(\"torch\")\n",
        "val_data = lm_datasets['validation'].with_format(\"torch\")\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "\n",
        "    # ix generates random indices from the dataset\n",
        "    ix = torch.randint(len(data), (batch_size,))\n",
        "\n",
        "    # The dataset now directly provides tokenized and blocked data\n",
        "    # We retrieve the 'input_ids' as 'x' and 'labels' as 'y'\n",
        "    batch = data[ix]\n",
        "    x = batch['input_ids'].to(device)\n",
        "    y = batch['labels'].to(device)\n",
        "\n",
        "    return x, y\n",
        "\n",
        "# Example: Get a batch (for testing)\n",
        "xb, yb = get_batch('train')\n",
        "print(xb.shape, yb.shape)"
      ],
      "metadata": {
        "id": "wCQPoWg5Z30K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NSkreAcO3V_",
        "outputId": "8703bf2a-72ee-43fb-b784-3055d32e5384"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-09-26 13:34:34--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2025-09-26 13:34:34 (40.3 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MoE(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple Mixture of Experts (top-1 routing).\n",
        "    - n_experts: number of expert FFNs\n",
        "    - expert_hidden: hidden dim for each expert (usually 4*n_embd)\n",
        "    - aux_loss_coef: coefficient for simple load balancing loss\n",
        "    \"\"\"\n",
        "    def __init__(self, n_embd, n_experts=4, expert_hidden=None, aux_loss_coef=1e-2):\n",
        "        super().__init__()\n",
        "        if expert_hidden is None:\n",
        "            expert_hidden = 4 * n_embd\n",
        "        self.n_experts = n_experts\n",
        "        self.n_embd = n_embd\n",
        "        self.aux_loss_coef = aux_loss_coef\n",
        "\n",
        "        # create expert modules (simple FFN)\n",
        "        self.experts = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(n_embd, expert_hidden),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(expert_hidden, n_embd)\n",
        "            ) for _ in range(n_experts)\n",
        "        ])\n",
        "\n",
        "        # gating network: produce scores per expert for each token\n",
        "        self.gate = nn.Linear(n_embd, n_experts)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, T, C)\n",
        "        B, T, C = x.shape\n",
        "        x_flat = x.view(B*T, C)  # (BT, C)\n",
        "\n",
        "        # gating logits -> probabilities\n",
        "        gate_logits = self.gate(x_flat)           # (BT, n_experts)\n",
        "        gate_probs = F.softmax(gate_logits, dim=-1)  # (BT, n_experts)\n",
        "\n",
        "        # top-1 route\n",
        "        top1_idx = torch.argmax(gate_probs, dim=-1)  # (BT,)\n",
        "        # one-hot dispatch mask (BT, n_experts)\n",
        "        dispatch_mask = F.one_hot(top1_idx, num_classes=self.n_experts).float()\n",
        "\n",
        "        # --- compute expert outputs by gathering assigned tokens per expert ---\n",
        "        # prepare output placeholder\n",
        "        outputs = torch.zeros_like(x_flat)  # (BT, C)\n",
        "        device = x_flat.device\n",
        "\n",
        "        expert_losses = []\n",
        "        for e, expert in enumerate(self.experts):\n",
        "            # find indices assigned to this expert\n",
        "            mask_e = (top1_idx == e)  # (BT,)\n",
        "            if mask_e.any():\n",
        "                x_e = x_flat[mask_e]            # (N_e, C)\n",
        "                y_e = expert(x_e)               # (N_e, C)\n",
        "                outputs[mask_e] = y_e\n",
        "            else:\n",
        "                # no tokens routed here; skip\n",
        "                continue\n",
        "\n",
        "        outputs = outputs.view(B, T, C)\n",
        "\n",
        "        # --- simple load balancing aux loss ---\n",
        "        # importance = sum of gate probs per expert, load = number of tokens chosen per expert\n",
        "        importance = gate_probs.sum(dim=0)            # (n_experts,)\n",
        "        load = dispatch_mask.sum(dim=0)               # (n_experts,)\n",
        "        # normalize and penalize deviation from uniform\n",
        "        importance_norm = importance / (importance.sum() + 1e-9)\n",
        "        load_norm = load / (load.sum() + 1e-9)\n",
        "        # squared deviation\n",
        "        aux_loss = ((importance_norm - (1.0/self.n_experts)).pow(2).mean()\n",
        "                    + (load_norm - (1.0/self.n_experts)).pow(2).mean())\n",
        "        aux_loss = aux_loss * self.aux_loss_coef\n",
        "\n",
        "        # return outputs and aux_loss (aux_loss is small scalar)\n",
        "        return outputs, aux_loss"
      ],
      "metadata": {
        "id": "3bUmXImbXorY"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = MoE(n_embd, n_experts=4, expert_hidden=4*n_embd, aux_loss_coef=1e-2)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        ff_out, ff_aux = self.ffwd(self.ln2(x))\n",
        "        x = x + ff_out\n",
        "        return x, ff_aux\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        # run through blocks, accumulating auxiliary MoE losses\n",
        "        total_aux_loss = 0.0\n",
        "        for block in self.blocks:\n",
        "            # if block is MoE-enabled, it returns (x, aux_loss)\n",
        "            out = block(x)\n",
        "            if isinstance(out, tuple):\n",
        "                x, aux_loss = out\n",
        "                total_aux_loss = total_aux_loss + aux_loss\n",
        "            else:\n",
        "                x = out\n",
        "\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            ce_loss = F.cross_entropy(logits, targets)\n",
        "            # add aux MoE loss\n",
        "            loss = ce_loss + total_aux_loss\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "model = model.to(device)\n",
        "m = torch.compile(model)\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss(model):\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss(m)\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoelkOrFY8bN",
        "outputId": "b13a7fee-e0e7-4af7-8f42-562911ba7561"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.607825 M parameters\n",
            "step 0: train loss 4.3679, val loss 4.3681\n",
            "step 100: train loss 2.6425, val loss 2.6571\n",
            "step 200: train loss 2.5153, val loss 2.5222\n",
            "step 300: train loss 2.4343, val loss 2.4386\n",
            "step 400: train loss 2.3731, val loss 2.3890\n",
            "step 500: train loss 2.3285, val loss 2.3297\n",
            "step 600: train loss 2.2852, val loss 2.2995\n",
            "step 700: train loss 2.2419, val loss 2.2681\n",
            "step 800: train loss 2.2045, val loss 2.2313\n",
            "step 900: train loss 2.1785, val loss 2.2027\n",
            "step 1000: train loss 2.1332, val loss 2.1704\n",
            "step 1100: train loss 2.1123, val loss 2.1552\n",
            "step 1200: train loss 2.0774, val loss 2.1168\n",
            "step 1300: train loss 2.0546, val loss 2.1066\n",
            "step 1400: train loss 2.0253, val loss 2.0788\n",
            "step 1500: train loss 1.9972, val loss 2.0657\n",
            "step 1600: train loss 1.9735, val loss 2.0449\n",
            "step 1700: train loss 1.9511, val loss 2.0506\n",
            "step 1800: train loss 1.9491, val loss 2.0413\n",
            "step 1900: train loss 1.9325, val loss 2.0117\n",
            "step 2000: train loss 1.9056, val loss 2.0005\n",
            "step 2100: train loss 1.8969, val loss 1.9957\n",
            "step 2200: train loss 1.8810, val loss 2.0041\n",
            "step 2300: train loss 1.8748, val loss 1.9825\n",
            "step 2400: train loss 1.8585, val loss 1.9826\n",
            "step 2500: train loss 1.8358, val loss 1.9611\n",
            "step 2600: train loss 1.8387, val loss 1.9609\n",
            "step 2700: train loss 1.8293, val loss 1.9496\n",
            "step 2800: train loss 1.8303, val loss 1.9435\n",
            "step 2900: train loss 1.8142, val loss 1.9481\n",
            "step 3000: train loss 1.8042, val loss 1.9427\n",
            "step 3100: train loss 1.7865, val loss 1.9196\n",
            "step 3200: train loss 1.7789, val loss 1.9176\n",
            "step 3300: train loss 1.7840, val loss 1.9158\n",
            "step 3400: train loss 1.7682, val loss 1.9226\n",
            "step 3500: train loss 1.7633, val loss 1.9004\n",
            "step 3600: train loss 1.7445, val loss 1.8959\n",
            "step 3700: train loss 1.7526, val loss 1.8861\n",
            "step 3800: train loss 1.7360, val loss 1.9061\n",
            "step 3900: train loss 1.7412, val loss 1.8985\n",
            "step 4000: train loss 1.7303, val loss 1.8847\n",
            "step 4100: train loss 1.7191, val loss 1.8648\n",
            "step 4200: train loss 1.7108, val loss 1.8747\n",
            "step 4300: train loss 1.7125, val loss 1.8662\n",
            "step 4400: train loss 1.7118, val loss 1.8527\n",
            "step 4500: train loss 1.7068, val loss 1.8524\n",
            "step 4600: train loss 1.6976, val loss 1.8468\n",
            "step 4700: train loss 1.6974, val loss 1.8604\n",
            "step 4800: train loss 1.6960, val loss 1.8457\n",
            "step 4900: train loss 1.6888, val loss 1.8323\n",
            "step 4999: train loss 1.6738, val loss 1.8316\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate from the model\n",
        "import time\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7xWj3RUe-5x",
        "outputId": "234da198-fbc2-4df3-ab7c-c2e09ff7427d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "And they brides.\n",
            "\n",
            "SOTRLOTES:\n",
            "Kind Peomb, you whaS greats that hause:\n",
            "Whither us hath but comelancase away, my fears\n",
            "acumzonour\n",
            "Yoursel'sful their vart; dill, at misters, hain;\n",
            "Sir, that GrEt, and Warrner one wars!\n",
            "Alaring that this much him sptup; and all,\n",
            "yet lord's know patelives; but you, then again Wilst our Ceiizase,\n",
            "Stillourion a guident to-disore-homb\n",
            "To king thrust for their hand: mowhe is expath,\n",
            "Madaried my offery his burs, you ar\n",
            "ards be his gentle up the king\n",
            "And kiry to-charmost! My fire youk,\n",
            "If you see, thy my mesore and see--'Sir;\n",
            "But with ready the custil son't weep one.\n",
            "\n",
            "Thun? Sevomats:\n",
            "Decks lord\n",
            "I issure to eirsurable I deabere over a mains!\n",
            "\n",
            "RAMILLO:\n",
            "I cam stie so upon thou fear, as nyther's,\n",
            "Why, knows hone duste tee, our hohearth.\n",
            "I'll againce's, with soul. Dive lore made so lack.\n",
            "\n",
            "Prive, must home,\n",
            "And I shark poir noth now thie,\n",
            "Thou hath diiusorthern, thee subgan appyter'd what thou the cusles,\n",
            "Be we king carrick, the Lorderward, time to tee, for Gly thee?\n",
            "\n",
            "Sere:\n",
            "Indeed, we soals, I hus night humber the varthes,\n",
            "And take all not weel such we see meen thinks and ears\n",
            "Singin it thou them Warch-then, chis burnes\n",
            "He blackenInired a canitious day, the anys\n",
            "In that lament theirer woes\n",
            "For with the shaw ye woung in tha detio your Hrowns too lust;\n",
            "Is there'er the fooljust the vow through'd,\n",
            "Shall and be love is captie.\n",
            "\n",
            "Provost:\n",
            "For they sick the be-daum!\n",
            "\n",
            "DUKE VORK:\n",
            "I with mutter bele hold; I the depiant om Three teny,\n",
            "Befely poor so in out so arnighle\n",
            "Of tell sment one thou that dow, say, as;\n",
            "Rest me of a-father my fatters, Aod wheresed well, sail\n",
            "My vereace abel?\n",
            "\n",
            "My shLory.\n",
            "\n",
            "BENVUCHIO:\n",
            "\n",
            "Forry:\n",
            "O, marrished arm that should distue\n",
            "In the hirsunedance ourg, hath see that lay not,\n",
            "There wifly, good may I heaven, thou tato though's our geash weat repeses befaid?\n",
            "But I hort but the peepose 'life?\n",
            "\n",
            "CLusice:\n",
            "Nay, to thee, our king-omfands, would poor and griar-neess,\n",
            "Bagaintumpen's though ston parforit brife shake's fight his but well washer be our lif\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet torch_tensorrt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uV9vKnh7A6x4",
        "outputId": "1492fd95-b6ad-4f3f-edb6-da870664bc2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/40.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.7/40.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.1/15.1 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for tensorrt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for tensorrt_cu12 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for tensorrt-cu12-libs (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TensorRT acceleration\n",
        "import torch_tensorrt\n",
        "\n",
        "class BigramInferenceWrapper(torch.nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "\n",
        "    def forward(self, idx):\n",
        "        idx = idx.to(torch.int32)  # TensorRT-safe\n",
        "        # patch positional indices\n",
        "        T = idx.shape[1]\n",
        "        B = idx.shape[0]\n",
        "        tok_emb = self.model.token_embedding_table(idx)  # (B,T,C)\n",
        "        pos_idx = torch.arange(T, device=idx.device, dtype=torch.int32)\n",
        "        pos_emb = self.model.position_embedding_table(pos_idx)\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.model.blocks(x)\n",
        "        x = self.model.ln_f(x)\n",
        "        logits = self.model.lm_head(x)\n",
        "        return logits\n",
        "\n",
        "model = model.to('cuda')\n",
        "inference_model = BigramInferenceWrapper(model)\n",
        "\n",
        "example_input = torch.zeros((1, 1), dtype=torch.int32, device='cuda')\n",
        "\n",
        "# Trace the wrapper model\n",
        "traced_model = torch.jit.trace(inference_model, example_input)\n",
        "traced_model.save(\"bigram_traced.pt\")"
      ],
      "metadata": {
        "id": "fjjvMifYZf7x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a32374ff-2663-4b89-a968-1d56a548b80c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torch_tensorrt.dynamo.conversion.aten_ops_converters:Unable to import quantization op. Please install modelopt library (https://github.com/NVIDIA/TensorRT-Model-Optimizer?tab=readme-ov-file#installation) to add support for compiling quantized models\n",
            "WARNING:torch_tensorrt.dynamo.conversion.aten_ops_converters:Unable to import quantize op. Please install modelopt library (https://github.com/NVIDIA/TensorRT-Model-Optimizer?tab=readme-ov-file#installation) to add support for compiling quantized models\n",
            "WARNING:torch_tensorrt.dynamo.conversion.converter_utils:TensorRT-LLM is not installed. Please install TensorRT-LLM or set TRTLLM_PLUGINS_PATH to the directory containing libnvinfer_plugin_tensorrt_llm.so to use converters for torch.distributed ops\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trt_model = torch_tensorrt.compile(\n",
        "    traced_model,\n",
        "    inputs=[torch_tensorrt.Input((1, block_size), dtype=torch.int32)],\n",
        "    enabled_precisions={torch.float32},\n",
        "    workspace_size=1 << 20,\n",
        "    truncate_long_and_double=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NC_RSWbSCBlf",
        "outputId": "85e5ed87-8c5f-4f60-a2c6-ea7d5dc6c4c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torch_tensorrt._compile:Input is a torchscript module but the ir was not specified (default=dynamo), please set ir=torchscript to suppress the warning.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def generate_trt(trt_model, idx, max_new_tokens, block_size, decode_fn):\n",
        "    idx = idx.to(torch.int32)  # Ensure input is int32 for TensorRT compatibility\n",
        "    for _ in range(max_new_tokens):\n",
        "        # Crop to the last block_size tokens\n",
        "        idx_cond = idx[:, -block_size:]  # Shape: (1, block_size)\n",
        "\n",
        "        # Ensure the input is the correct shape\n",
        "        if idx_cond.shape[1] < block_size:\n",
        "            # Pad with zeros if the sequence is shorter than block_size\n",
        "            padding = torch.zeros((idx_cond.shape[0], block_size - idx_cond.shape[1]),\n",
        "                                dtype=torch.int32, device=idx.device)\n",
        "            idx_cond = torch.cat((padding, idx_cond), dim=1)\n",
        "        idx_cond = idx_cond.to(torch.int32)  # Shape: (1, block_size)\n",
        "\n",
        "        # Run TensorRT model\n",
        "        logits = trt_model(idx_cond)  # Shape: (1, block_size, vocab_size)\n",
        "\n",
        "        # Use the logits for the last time step\n",
        "        logits = logits[:, -1, :]  # Shape: (1, vocab_size)\n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "        idx_next = torch.multinomial(probs, num_samples=1)  # Shape: (1, 1)\n",
        "\n",
        "        # Append new token\n",
        "        idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "    return decode_fn(idx[0].tolist())\n",
        "\n",
        "context = torch.zeros((1, 1), dtype=torch.int32, device='cuda')\n",
        "generated_text = generate_trt(trt_model, context, max_new_tokens=500, block_size=32, decode_fn=decode)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PceP6eDVDDox",
        "outputId": "864a924d-7aca-4c3e-dde8-6566f31f5bbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "SLERSCUMEREY:\n",
            "Let is not not you wast. But, and my virture to controse wove?\n",
            "\n",
            "Secomfort, firsts osys have?\n",
            "\n",
            "HORTIONA:\n",
            "O, I screts I some.\n",
            "I is Alme my notly have beyeing make:\n",
            "And I shame itto me your clannst of\n",
            "my sounk, silewiman our sweenger to-discoustractar wames\n",
            "Wetcher him you.\n",
            "Ummon teat the sadlingna\n",
            "To carse, whose been misely opge,\n",
            "And there such but kind this is guryanny to purt butwere my warre morries\n",
            "Both loves your wonds it hate.\n",
            "\n",
            "LUCENTIO:\n",
            "WhavEN thrus than to my unaght, this sw\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As seen below, TensorRT is about 8x faster."
      ],
      "metadata": {
        "id": "4PLA0ELKiPr9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Benchmark TensorRT\n",
        "start_time = time.time()\n",
        "_ = generate_trt(trt_model, context, max_new_tokens=2000, block_size=32, decode_fn=decode)\n",
        "end_time = time.time()\n",
        "print(end_time - start_time)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mns8ixkSfkq0",
        "outputId": "fbe2b5d1-a571-4167-ddbf-9ebb541b84a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.6884980201721191\n"
          ]
        }
      ]
    }
  ]
}